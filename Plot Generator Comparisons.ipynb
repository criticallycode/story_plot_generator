{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook examines several different ways of generating text. I wanted to create a plot generator, something that would generate story ideas based on different genres. This notebook displays the various ways I tried to accomplish this text generation: hard-coded text combination, an LSTM deep learning model, a RNN model based on  TextGenRNN, a custom Markov chain model, and a model based on Markovify. You can copy the notebook and used the files included in the repo to experiment with the results of the different text generation methods. \n",
    "\n",
    "All credits for algorithms found in the repo's README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, I tried hard-coding a function to generate text. This function combines pre-defined terms together to generate text. The theme here is \"Sci-fi\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a city under the ocean, there is a male engineer who seeks revenge against internet trolls.\n",
      "In the heart of Silicon Valley, there is a female soldier who exceeded beyond a corrupt government.\n",
      "In a massive underground facility, there is a female writer who tries to befriend a virus.\n",
      "In a virtual world, there is a robot hacker who exceeded beyond a corrupt government.\n",
      "In the heart of Silicon Valley, there is a male engineer who exceeded beyond a powerful street gang.\n",
      "In future Tokyo, there is a male writer who flees from a massive corporation.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def plot_gen(num_gen):\n",
    "    i = 0\n",
    "    while i <= num_gen:\n",
    "        setting = random.choice(\n",
    "                [\"future Tokyo\", \"future New York\", \"a utopia\", \"a dystopia\", \"a virtual world\", \"a base on the Moon\",\n",
    "                 \"the heart of Silicon Valley\", \"a city under the ocean\", \"a massive underground facility\", \"an artificial island\"])\n",
    "        gender = random.choice(\n",
    "                [\"male \", \"male \", \"female \", \"female \", \"robot \", \"third gender \"])\n",
    "        classs = random.choice(\n",
    "                [\"hacker\", \"cyborg\", \"engineer\", \"corporate employee\", \"street rat\", \"soldier\", \"doctor\", \"detective\", \"pilot\", \"writer\"])\n",
    "        protagonist = gender + classs\n",
    "        antagonist = random.choice(\n",
    "                [\"a massive corporation\", \"a rogue AI\", \"a powerful street gang\", \"a secret society\", \"a disruptive technology\", \"robots\",\n",
    "                 \"internet trolls\", \"a virus\", \"a group of aliens\", \"a corrupt government\", \"new pirates/bandits\"])\n",
    "        conflict = random.choice(\n",
    "            [\"falls in love with \", \"attempts to stop \", \"fights against \", \"flees from \", \"exceeded beyond \", \"defends against \", \"tries to befriend \",\n",
    "             \"explores with \", \"competes with \", \"seeks revenge against \"])\n",
    "        print(\"In\" + \" \" + setting + \", there is a\" + \" \" + protagonist + \" \" + \"who\" + \" \" + conflict + antagonist + \".\")\n",
    "        i += 1\n",
    "\n",
    "plot_gen(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique produced acceptable results, but it was very limited. For this reason, I wanted to create a generation function that was more robust. I turned to deep learning to accomplish this. After collecting a bunch of plots of horror movies from the Open Movie Database, this was the code I used to train the model. I used a Long Short-Term Memory model, which excels at generating text. \n",
    "\n",
    "First, the text is loaded in and two dictionaries are created. The first dictionary maps text characters to numerical values, while the second dictionary maps numerical values to characters. This allows us to encode and decode the text data for use by the model. After this, the input sentences and the succeeding characters must be defined. Afterwards, the X/features are Y/labels are created for the model using the defined varaibles.\n",
    "\n",
    "After this, an optimizer for the model is selected. The model is then created and instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length 244182\n",
      "a meteor strikes a houseboat in the swamps near a southern town populated by yankees with fake accents. the people on the houseboat become zombies who feed on the alligators in the swamp. ...\n",
      "\"a baby alligator is flushed down a chicago toilet and survives by eating discarded laboratory rats injected\n",
      "total chars:  61\n",
      "Number of sequences: 81378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1018 21:09:51.357419 24504 deprecation_wrapper.py:119] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1018 21:09:51.358415 24504 deprecation_wrapper.py:119] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1018 21:09:51.361383 24504 deprecation_wrapper.py:119] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1018 21:09:52.048580 24504 deprecation_wrapper.py:119] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1018 21:09:52.054563 24504 deprecation.py:506] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1018 21:09:52.127335 24504 deprecation_wrapper.py:119] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1018 21:09:52.134349 24504 deprecation_wrapper.py:119] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LambdaCallback, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "text = open('horror_plots_2_correct.txt', 'r').read().lower()\n",
    "print('text length', len(text))\n",
    "\n",
    "print(text[:300])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars: ', len(chars))\n",
    "\n",
    "# Map characters to numeric values and vice-versa\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "num_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "max_length = 50\n",
    "step = 3\n",
    "sentences = []\n",
    "next = []\n",
    "\n",
    "# Create input sentences for the model, as well as the next character in the sequence\n",
    "for i in range(0, len(text) - max_length, step):\n",
    "    sentences.append(text[i: i + max_length])\n",
    "    next.append(text[i + max_length])\n",
    "\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# Create features and labels for the model\n",
    "# Create numpy array full of zeroes\n",
    "x = np.zeros((len(sentences), max_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "# Fill in array values\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    for i, char in enumerate(sentence):\n",
    "        x[idx, i, char_to_num[char]] = 1\n",
    "    y[idx, char_to_num[next[idx]]] = 1\n",
    "\n",
    "# Declare an optimizer for the model\n",
    "optim = Adam(lr=0.01)\n",
    "\n",
    "# Create the model format\n",
    "def create_model(max_len, charas, optim):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(max_len, len(charas)), return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(charas)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the model instance\n",
    "model = create_model(max_length, chars, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functions needed for the generation of text. The first function gets the probability values for the next characters in the sequence, the predictions. The second function prints the text at the end of every 5 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # pull an index from probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    # get the log probability\n",
    "    preds = np.log(preds) / temperature\n",
    "    # expand the array\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    # create a multinomial distribution based on the preds and sample from it\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    # get the max (highest probability) prediction\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def end_epoch(epoch, logs):\n",
    "    # function runs every 5 epochs\n",
    "    # prints a string of text text_created with the current model parameters\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "\n",
    "        print()\n",
    "        print('Generation results after epoch {}'.format(epoch))\n",
    "        print()\n",
    "        # Set an initial index to start our generation on by randomly selecting an integer\n",
    "        initial_idx = random.randint(0, len(text) - max_length - 1)\n",
    "        # Generate the characters for our list of different divesrsities\n",
    "        for diversity in [0.5, 0.75, 1.0, 1.25]:\n",
    "            print('Current diversity: {}'.format(diversity))\n",
    "            # Reference the starting index and get the sentence that follows\n",
    "            # This sentence is what will be used to generate the text\n",
    "            text_created = ''\n",
    "            sentence = text[initial_idx: initial_idx + max_length]\n",
    "            print(\"Seed to generate from:\".format(sentence))\n",
    "            text_created += sentence\n",
    "            sys.stdout.write(text_created)\n",
    "            print(\" \")\n",
    "            print(\"Generated text:\")\n",
    "            print(\"___\")\n",
    "\n",
    "            for i in range(500):\n",
    "\n",
    "                # Construct an array of zeros to fit the predictions into\n",
    "                feature_pred = np.zeros((1, max_length, len(chars)))\n",
    "\n",
    "                # Fill in the feature array with the numbers that represent characters\n",
    "                for n, char in enumerate(sentence):\n",
    "                    feature_pred[0, n, char_to_num[char]] = 1.\n",
    "\n",
    "                # Use the model to predict based off of the currents features\n",
    "                preds = model.predict(feature_pred, verbose=0)[0]\n",
    "                # get the most likely prediction from the predictions list\n",
    "                next_idx = sample(preds, diversity)\n",
    "                # Convert the index to an actual character\n",
    "                next_char = num_to_char[next_idx]\n",
    "\n",
    "                # Add the character to the string to be text_created\n",
    "                text_created += next_char\n",
    "                # Move on to the next character in the sentence\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                # Start compiling the list of probable next characters based on sample predictions\n",
    "                sys.stdout.write(next_char)\n",
    "                # Write the characters to the terminal\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            print(\" \")\n",
    "            print(\"___\")\n",
    "            print(\" \")\n",
    "\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now go about defining some callbacks for the training, as well as a filepath for the saved weights. We can then fit/train our model and capture some metrics like loss and accuracy. We'll then visualize these metrics after training is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 21:09:52.355729 24504 deprecation.py:323] From c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "70144/81378 [========================>.....] - ETA: 20s - loss: 2.4310 - acc: 0.3022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a541cfeda0f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m             EarlyStopping(monitor= 'loss', min_delta=1e-10, patience=15, verbose=1, restore_best_weights=True)]\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=end_epoch)\n",
    "\n",
    "filepath = \"weights.hdf5\"\n",
    "\n",
    "callbacks = [print_callback,\n",
    "            ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min'),\n",
    "            ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, verbose=1, mode='min', min_lr=0.00001),\n",
    "            EarlyStopping(monitor= 'loss', min_delta=1e-10, patience=15, verbose=1, restore_best_weights=True)]\n",
    "\n",
    "records = model.fit(x, y, batch_size=128, epochs=100, callbacks=callbacks)\n",
    "\n",
    "t_loss = records.history['loss']\n",
    "t_acc = records.history['acc']\n",
    "\n",
    "# gets the lengt of how long the model was trained for\n",
    "train_length = range(1, len(t_loss) + 1)\n",
    "\n",
    "def evaluation(model, train_length, training_loss, training_acc):\n",
    "\n",
    "    # plot the loss across the number of epochs\n",
    "    plt.figure()\n",
    "    plt.plot(train_length, training_loss, label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_length, training_acc, 'r', label='Training acc')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.show()\n",
    "\n",
    "    # compare against the test training set\n",
    "    # get the score/accuracy for the current model\n",
    "    scores = model.evaluate(x, y, batch_size=128)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "# evaluation(model, train_length, t_loss, t_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a function to generate text, taking in the saved weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen():\n",
    "\n",
    "    model.load_weights(\"weights.hdf5\")\n",
    "\n",
    "    initial_idx = random.randint(0, len(text) - max_length - 1)\n",
    "    # Generate the characters for our list of different divesrsities\n",
    "    for diversity in [0.5, 0.75, 1.0, 1.25]:\n",
    "        print('Current diversity: {}'.format(diversity))\n",
    "        # Reference the starting index and get the sentence that follows\n",
    "        # This sentence is what will be used to generate the text\n",
    "        text_created = ''\n",
    "        sentence = text[initial_idx: initial_idx + max_length]\n",
    "        print(\"Seed to generate from:\".format(sentence))\n",
    "        text_created += sentence\n",
    "        sys.stdout.write(text_created)\n",
    "        print(\" \")\n",
    "        print(\"Generated text:\")\n",
    "        print(\"___\")\n",
    "        for i in range(500):\n",
    "\n",
    "            # Construct an array of zeros to fit the predictions into\n",
    "            feature_pred = np.zeros((1, max_length, len(chars)))\n",
    "\n",
    "            # Fill in the feature array with the numbers that represent characters\n",
    "            for n, char in enumerate(sentence):\n",
    "                feature_pred[0, n, char_to_num[char]] = 1.\n",
    "\n",
    "            # Use the model to predict based off of the currents features\n",
    "            preds = model.predict(feature_pred, verbose=0)[0]\n",
    "            # get the most likely prediction from the predictions list\n",
    "            next_idx = sample(preds, diversity)\n",
    "            # Convert the index to an actual character\n",
    "            next_char = num_to_char[next_idx]\n",
    "\n",
    "            # Add the character to the string to be text_created\n",
    "            text_created += next_char\n",
    "            # Move on to the next character in the sentence\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            # Start compiling the list of probable next characters based on sample predictions\n",
    "            sys.stdout.write(next_char)\n",
    "            # Write the characters to the terminal\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print(\" \")\n",
    "        print(\"___\")\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results I got from this where underwheleming. Even after 100 epochs of training and minimal loss, the text that was generated wasn't very coherent. Ultimately I probably need more training data for this approach, and to spend more time experimenting with the model.\n",
    "\n",
    "More training data and training for more epochs might help, but I began to wonder if there was an easier way to accomplish my goal. I found out about the TextGenRNN model referenced in the README attached to the repo and tried implementing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "\n",
    "input_file = \"horror_plots.txt\"\n",
    "epochs = 50\n",
    "weights_file = \"textgenrnn_weights.hdf5\"\n",
    "\n",
    "def train_generator(input_file, epochs=0):\n",
    "    textgen_model = textgenrnn()\n",
    "    textgen_model.train_from_file(input_file, num_epochs=epochs)\n",
    "    textgen_model.save(\"textgenrnn_weights.hdf5\")\n",
    "\n",
    "#train_generator(input_file, epochs)\n",
    "\n",
    "def gen_text(weights_file):\n",
    "    textgen_model = textgenrnn()\n",
    "    textgen_model.load(weights_file)\n",
    "    textgen_model.generate()\n",
    "\n",
    "gen_text(weights_file=weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text genreated by this library was of higher quality, but it still wasn't quite what I was looking for. Training also took quite a bit of time. I investigated Markov chains as a potential solution to my problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is an implementation of a Markov chain for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "class Markov(object):\n",
    "\n",
    "    def __init__(self, order):\n",
    "\n",
    "        # order refers to how far back the process will look or remember\n",
    "\n",
    "        self.order = order\n",
    "\n",
    "        # controls the actual size of the word groups to be analyzed\n",
    "        self.group_size = self.order + 1\n",
    "\n",
    "        # the training text\n",
    "\n",
    "        self.text = None\n",
    "\n",
    "        #graph dictionary will hold the actual information\n",
    "        self.graph = {}\n",
    "\n",
    "        return\n",
    "\n",
    "    def train(self, filename):\n",
    "        self.text = filename.read().split()\n",
    "\n",
    "        # this appends the beginning of the text to the end of the text\n",
    "        # so that it always has something to generate\n",
    "        self.text = self.text + self.text[:self.order]\n",
    "\n",
    "        # iterate one by one over text, for the entire range of the text starting\n",
    "        # from word 0 to the last possible groups of word\n",
    "        for i in range(0, len(self.text) - self.group_size):\n",
    "\n",
    "            # key is the few words that came before the value\n",
    "            key = tuple(self.text[i:i + self.order])\n",
    "            # value is the word that is coming up now, final word in the sequence\n",
    "            # order 2 markov chain will have value be word 3\n",
    "            value = self.text[i + self.order]\n",
    "\n",
    "            # if the word has already been seen, just append the value to the end of the dict\n",
    "            if key in self.graph:\n",
    "                self.graph[key].append(value)\n",
    "            # if word hasn't been seen before, just add it to value of\n",
    "            # all words we've seen come after specific word pair\n",
    "            # save the data\n",
    "            else:\n",
    "                self.graph[key] = [value]\n",
    "\n",
    "    def generate(self, length):\n",
    "\n",
    "        # index defines where the text generation begins at, picks a randomn start word\n",
    "        index = random.randint(0, len(self.text) - self.order)\n",
    "\n",
    "        # result comes after the randomly chosen word\n",
    "        result = self.text[index: index + self.order]\n",
    "\n",
    "        for i in range(length):\n",
    "\n",
    "            # current state is the last few words of the current result\n",
    "            state = tuple(result[len(result) - self.order:])\n",
    "            # next word is randomly chosen from possible values in the graph\n",
    "            next_word = random.choice(self.graph[state])\n",
    "            # append the value to the result\n",
    "            result.append(next_word)\n",
    "\n",
    "        print(\" \".join(result[self.order:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the model generator. The generate function takes the amount of character to generate, while the declaration of the generator takes the order/length to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_data = open(\"Horror_plots.txt\")\n",
    "generator = Markov.Markov(2)\n",
    "generator.train(markov_data)\n",
    "print(\"Basic Markov model generated:\")\n",
    "generator.generate(30)\n",
    "print(\"_______\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I hit on a solution that seemed to work well for what I was trying accomplish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import en_core_web_sm\n",
    "\n",
    "input_text = open('Horror_plots.txt').read()\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# regular markovify\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(input_text, state_size=2)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "print(\"Vanilla Markovify:\")\n",
    "print(\"---\")\n",
    "print(\"Sentence Gen:\")\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Short sentence gen:\")\n",
    "# Print three randomly-generated sentences of no more than 140 characters\n",
    "for i in range(10):\n",
    "    print(text_model.make_short_sentence(140))\n",
    "\n",
    "print(\"________\")\n",
    "\n",
    "#sentence_gen()\n",
    "\n",
    "# overwrite default markovify model\n",
    "\n",
    "class POSText(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return [\"::\".join((word.orth_, word.pos_)) for word in nlp(sentence)]\n",
    "\n",
    "    def word_join(self, words):\n",
    "        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n",
    "        return sentence\n",
    "\n",
    "text_model2 = POSText(input_text, state_size=3)\n",
    "\n",
    "print(\"Modified Markov model:\")\n",
    "print(\"---\")\n",
    "print(\"Markov full sentence gen:\")\n",
    "print(\" \")\n",
    "for i in range(10):\n",
    "    print(text_model2.make_sentence())\n",
    "print(\"________\")\n",
    "\n",
    "print(\"Markov short sentence gen:\")\n",
    "print(\" \")\n",
    "for i in range(10):\n",
    "    print(text_model2.make_short_sentence(150))\n",
    "print(\"________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
